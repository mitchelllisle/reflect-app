{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import json\n",
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "no_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_word_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperp_tol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_change_tol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_update_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Latent Dirichlet Allocation with online variational Bayes algorithm\n",
       "\n",
       ".. versionadded:: 0.17\n",
       "\n",
       "Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_components : int, optional (default=10)\n",
       "    Number of topics.\n",
       "\n",
       "doc_topic_prior : float, optional (default=None)\n",
       "    Prior of document topic distribution `theta`. If the value is None,\n",
       "    defaults to `1 / n_components`.\n",
       "    In the literature, this is called `alpha`.\n",
       "\n",
       "topic_word_prior : float, optional (default=None)\n",
       "    Prior of topic word distribution `beta`. If the value is None, defaults\n",
       "    to `1 / n_components`.\n",
       "    In the literature, this is called `beta`.\n",
       "\n",
       "learning_method : 'batch' | 'online', default='batch'\n",
       "    Method used to update `_component`. Only used in `fit` method.\n",
       "    In general, if the data size is large, the online update will be much\n",
       "    faster than the batch update.\n",
       "\n",
       "    Valid options::\n",
       "\n",
       "        'batch': Batch variational Bayes method. Use all training data in\n",
       "            each EM update.\n",
       "            Old `components_` will be overwritten in each iteration.\n",
       "        'online': Online variational Bayes method. In each EM update, use\n",
       "            mini-batch of training data to update the ``components_``\n",
       "            variable incrementally. The learning rate is controlled by the\n",
       "            ``learning_decay`` and the ``learning_offset`` parameters.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "        The default learning method is now ``\"batch\"``.\n",
       "\n",
       "learning_decay : float, optional (default=0.7)\n",
       "    It is a parameter that control learning rate in the online learning\n",
       "    method. The value should be set between (0.5, 1.0] to guarantee\n",
       "    asymptotic convergence. When the value is 0.0 and batch_size is\n",
       "    ``n_samples``, the update method is same as batch learning. In the\n",
       "    literature, this is called kappa.\n",
       "\n",
       "learning_offset : float, optional (default=10.)\n",
       "    A (positive) parameter that downweights early iterations in online\n",
       "    learning.  It should be greater than 1.0. In the literature, this is\n",
       "    called tau_0.\n",
       "\n",
       "max_iter : integer, optional (default=10)\n",
       "    The maximum number of iterations.\n",
       "\n",
       "batch_size : int, optional (default=128)\n",
       "    Number of documents to use in each EM iteration. Only used in online\n",
       "    learning.\n",
       "\n",
       "evaluate_every : int, optional (default=0)\n",
       "    How often to evaluate perplexity. Only used in `fit` method.\n",
       "    set it to 0 or negative number to not evalute perplexity in\n",
       "    training at all. Evaluating perplexity can help you check convergence\n",
       "    in training process, but it will also increase total training time.\n",
       "    Evaluating perplexity in every iteration might increase training time\n",
       "    up to two-fold.\n",
       "\n",
       "total_samples : int, optional (default=1e6)\n",
       "    Total number of documents. Only used in the `partial_fit` method.\n",
       "\n",
       "perp_tol : float, optional (default=1e-1)\n",
       "    Perplexity tolerance in batch learning. Only used when\n",
       "    ``evaluate_every`` is greater than 0.\n",
       "\n",
       "mean_change_tol : float, optional (default=1e-3)\n",
       "    Stopping tolerance for updating document topic distribution in E-step.\n",
       "\n",
       "max_doc_update_iter : int (default=100)\n",
       "    Max number of iterations for updating document topic distribution in\n",
       "    the E-step.\n",
       "\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    The number of jobs to use in the E-step.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "verbose : int, optional (default=0)\n",
       "    Verbosity level.\n",
       "\n",
       "random_state : int, RandomState instance or None, optional (default=None)\n",
       "    If int, random_state is the seed used by the random number generator;\n",
       "    If RandomState instance, random_state is the random number generator;\n",
       "    If None, the random number generator is the RandomState instance used\n",
       "    by `np.random`.\n",
       "\n",
       "n_topics : int, optional (default=None)\n",
       "    This parameter has been renamed to n_components and will\n",
       "    be removed in version 0.21.\n",
       "    .. deprecated:: 0.19\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "components_ : array, [n_components, n_features]\n",
       "    Variational parameters for topic word distribution. Since the complete\n",
       "    conditional for topic word distribution is a Dirichlet,\n",
       "    ``components_[i, j]`` can be viewed as pseudocount that represents the\n",
       "    number of times word `j` was assigned to topic `i`.\n",
       "    It can also be viewed as distribution over the words for each topic\n",
       "    after normalization:\n",
       "    ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n",
       "\n",
       "n_batch_iter_ : int\n",
       "    Number of iterations of the EM step.\n",
       "\n",
       "n_iter_ : int\n",
       "    Number of passes over the dataset.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.decomposition import LatentDirichletAllocation\n",
       ">>> from sklearn.datasets import make_multilabel_classification\n",
       ">>> # This produces a feature matrix of token counts, similar to what\n",
       ">>> # CountVectorizer would produce on text.\n",
       ">>> X, _ = make_multilabel_classification(random_state=0)\n",
       ">>> lda = LatentDirichletAllocation(n_components=5,\n",
       "...     random_state=0)\n",
       ">>> lda.fit(X) # doctest: +ELLIPSIS\n",
       "LatentDirichletAllocation(...)\n",
       ">>> # get topics for some given samples:\n",
       ">>> lda.transform(X[-2:])\n",
       "array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n",
       "       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n",
       "\n",
       "References\n",
       "----------\n",
       "[1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D. Hoffman,\n",
       "    David M. Blei, Francis Bach, 2010\n",
       "\n",
       "[2] \"Stochastic Variational Inference\", Matthew D. Hoffman, David M. Blei,\n",
       "    Chong Wang, John Paisley, 2013\n",
       "\n",
       "[3] Matthew D. Hoffman's onlineldavb code. Link:\n",
       "    https://github.com/blei-lab/onlineldavb\n",
       "\u001b[0;31mFile:\u001b[0m           /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LatentDirichletAllocation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people time right did good said say make way government\n",
      "Topic 1:\n",
      "window problem using server application screen display motif manager running\n",
      "Topic 2:\n",
      "god jesus bible christ faith believe christian christians sin church\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info help information address appreciated\n",
      "Topic 6:\n",
      "windows file files dos program version ftp ms directory running\n",
      "Topic 7:\n",
      "edu soon cs university ftp internet article email pub david\n",
      "Topic 8:\n",
      "key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 9:\n",
      "drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just ll thought tell oh little fine work wanted mean\n",
      "Topic 11:\n",
      "does know anybody mean work say doesn help exist program\n",
      "Topic 12:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 13:\n",
      "like sounds looks look bike sound lot things really thing\n",
      "Topic 14:\n",
      "don know want let need doesn little sure sorry things\n",
      "Topic 15:\n",
      "car cars engine speed good bike driver road insurance fast\n",
      "Topic 16:\n",
      "ve got seen heard tried good recently times try couple\n",
      "Topic 17:\n",
      "use used using work available want software need image data\n",
      "Topic 18:\n",
      "think don lot try makes really pretty wasn bit david\n",
      "Topic 19:\n",
      "com list dave internet article sun hp email ibm phone\n",
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "with open(\"/Users/mitchell/Downloads/reviews_Musical_Instruments_5.json\") as f:\n",
    "    data = f.readlines()\n",
    "    for row in data:\n",
    "        out.append(json.loads(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"reviewerID\": \"A2IBPI20UZIR0U\", \"asin\": \"1384719342\", \"reviewerName\": \"cassandra tu \\\\\"Yeah, well, that\\'s just like, u...\", \"helpful\": [0, 0], \"reviewText\": \"Not much to write about here, but it does exactly what it\\'s supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\", \"overall\": 5.0, \"summary\": \"good\", \"unixReviewTime\": 1393545600, \"reviewTime\": \"02 28, 2014\"}\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for row in out:\n",
    "    assign = np.random.choice([\"GOOD\", \"WONDERING\", \"BAD\"], p=[0.3, 0.3, 0.4])\n",
    "    d.append({\n",
    "        \"id\": str(uuid4()),\n",
    "        \"project_name\": \"handsome-coyote\",\n",
    "        \"text\": re.sub(r\"\\d+\", \"\", row[\"reviewText\"][:100]),\n",
    "        \"type\": assign\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(d).head(200).to_csv(\"~/Desktop/review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
